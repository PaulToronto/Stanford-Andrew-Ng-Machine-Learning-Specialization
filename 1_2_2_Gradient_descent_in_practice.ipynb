{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzBTVZXTttNi5M6kfdMCiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/Stanford-Andrew-Ng-Machine-Learning-Specialization/blob/main/1_2_2_Gradient_descent_in_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2.2 Gradient descent in practice"
      ],
      "metadata": {
        "id": "mWM5zG2TYgrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.1 Feature scaling"
      ],
      "metadata": {
        "id": "8T7DV4YiYnBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature and parameter values\n",
        "\n",
        "$$\n",
        "\\widehat{price} = w_1x_1 + w_2x_2 + b\n",
        "$$\n",
        "\n",
        "- price is in \\$1000s\n",
        "- $x_1$ represents the size of the house in $\\text{feet}^{2}$\n",
        " - range: 300 to 2,000 square feet\n",
        " - relatively **large range** of values\n",
        "- $x_2$ represents the nunber of bedrooms\n",
        " - range: 0 to 5 bedrooms\n",
        " - relatively **small range** of values\n",
        "\n",
        "#### Example House\n",
        "\n",
        "- House: $x_1 = 2000$, $x_2 = 5$, $price = 500$\n",
        "- $b = 50$\n",
        "- Possible parameters: $w_1 = 50, w_2 = 0.1$\n",
        " - $\\widehat{price} = 100,050,500$\n",
        " - clearly, not a good choice for $w_1$ and $w_2$\n",
        "- Possibe parameters: $w_1 = 0.1, w_2 = 50$\n",
        " - $\\widehat{price} = 500,000$\n",
        " - a perfect choice for $w_1$ and $w_2$\n",
        "\n",
        "#### What does this show?\n",
        "\n",
        "- When a feature has a relatively large range, there is a good chance that the optimal weight for that feature will be a small value.\n",
        "- Likewise, when the possible values of feature are small, there is a good change that the optimal weight for that feature will be a large value.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmrJ7do_-d4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does this relate to gradient descent?\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=198UySZdh8xm9dyfEWXQOzogdz1Gxdbh7'>"
      ],
      "metadata": {
        "id": "mCBd2HkxIVwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Notice that the contours form ellipses, not circles\n",
        "- Because the contours are so tall and skinny, gradient descent may end up bouncing back and forth for a long time before it can finally find its way to the global minimum\n",
        "- **Feature scaling** enables gradient descent to find a much more direct path to the global minimum."
      ],
      "metadata": {
        "id": "656iSjm0LiWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1zKhYXV1GtOJxBxjUrRgmp1nHyv1kkWn0'>"
      ],
      "metadata": {
        "id": "0Olgvd3iMpWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to implement scaling\n",
        "\n",
        "#### Method 1: Divide each value by the maximum\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "300 \\le x_1 \\le 2000 &\\rightarrow 0.15 \\le x_1 \\le 1 \\\\\n",
        "0 \\le x_2 \\le 5 &\\rightarrow 0 \\le x_2 \\le 1\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "#### Method 2: Mean normalization (centres it at 0)\n",
        "\n",
        "$$\n",
        "x_j = \\frac{x_j - \\mu_j}{max(x_j) - min(x_j)}\n",
        "$$\n",
        "\n",
        "#### Mathod 3: Z-score normalization\n",
        "\n",
        "$$\n",
        "x_j = \\frac{x_j - \\mu_j}{\\sigma_1}\n",
        "$$"
      ],
      "metadata": {
        "id": "opLMVZ5GOTc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling Examples\n",
        "\n",
        "- aim for about $-1 \\le x_j \\le 1$ for each feature $j$\n",
        "- the following are all ok:\n",
        " - $-3 \\le x_j \\le 3$\n",
        " - $-0.3 \\le x_j \\le 0.3$\n",
        " - $0 \\le x_j \\le 3$\n",
        " - $-2 \\le x_j \\le 0.5$\n",
        "- not ok, should rescale\n",
        " - $-100 \\le x_3 \\le 100$, too large\n",
        " - $-0.0001 \\le x_4 \\le 0.0001$, too small\n",
        " - $98.6 \\le x_5 \\le 105$, too large"
      ],
      "metadata": {
        "id": "A0f2YyuiSafN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.2 Checking gradient descent for convergence"
      ],
      "metadata": {
        "id": "LMVMlnk9UyYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient descent\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "w_j &= w_j - \\alpha \\frac{\\partial}{\\partial w_j}J\\left(\\vec{w}, b\\right) \\\\\n",
        "b &= b - \\alpha \\frac{\\partial}{\\partial b}J\\left(\\vec{w}, b\\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "- One of the key choices is the choice of the learning rate, $\\alpha$."
      ],
      "metadata": {
        "id": "3mRc8RhWCpxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make sure that gradient descent is working correctly\n",
        "\n",
        "objective: $\\min_{\\vec{w},b}J(\\vec{w},b)$\n",
        "\n",
        "- one strategy is to plot **$J(\\vec{w},b)$ vs. # interations**\n",
        "- this curve is called a **learning curve**\n",
        "- there are a few different types of learning curves used in machine learning\n",
        "- if gradient descent is working properly, $J(\\vec{w},b)$ should decrease after each iteration\n",
        "- if J ever increases after one iteration, that means $\\alpha$ is chosen poorly\n",
        " - usually means $\\alpha$ is too large\n",
        " - or there could be a bug in the code\n",
        "- As the number of iterations increases, J should start to level off\n",
        " - that means gradient descent has **coverged**\n",
        "\n"
      ],
      "metadata": {
        "id": "PuEt0VOdEgtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic convergence test\n",
        "\n",
        "Let $\\epsilon$ be $10^{-3}$ or some other small number.\n",
        "\n",
        "If $J(\\vec{w},b)$ decreases by $\\le \\epsilon$ in one iteration, declare **convergence**."
      ],
      "metadata": {
        "id": "2-Knx2TdJ2Yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.3 Choosing the learning rate\n",
        "\n",
        "- If you plot the learning curve and notice the cost goes up and down, that is a good indication that there is a bug in your code, or that the learning rate is too large\n",
        "    - Try setting a smaller learning rate\n",
        "- Sometimes you will see a learning curve where the cost is consistently increasing, could also be a learning rate that it too large, or could be a bug in your code\n",
        "    - debugging tip: with a small enough learning rate, $J(\\vec{w},b)$ should descrease on every iteration, so set $\\alpha$ to a very small number for testing\n",
        "- values of $\\alpha$ to try:\n",
        " - $\\cdots 0.001, 0.01, 0.1, 1, \\cdots$\n",
        " - or roughly 3 times larger each time: $\\cdots 0.001, 0.003, .01, 0.03, 0.1, 0.3, 1, \\cdots$\n",
        " - start with the smaller one and run gradient descent for a handful of iterations, then try a larger one, with the goal of finding $\\alpha$ such that the cost descreases rapidly, but also consistently"
      ],
      "metadata": {
        "id": "sUxIMrAcKxCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.4 Lab - Feature Scaling and Learning Rate\n",
        "\n",
        "https://colab.research.google.com/drive/1OH2UCFia4HRa9sXWvin6m84v2wQKCXDb"
      ],
      "metadata": {
        "id": "-L4zBYWBai_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.5 Feature Engineering\n",
        "\n",
        "Say you have two features to predict the price of a house:\n",
        "\n",
        "1. $x_1$ is the *frontage* or width of the lot\n",
        "2. $x_2$ is the *depth* of the rectangular lot\n",
        "\n",
        "$$\n",
        "f_{\\vec{w},b} = w_1x_1 + w_2x_2 + b\n",
        "$$\n",
        "\n",
        "This model might work well, but there is another option.\n",
        "\n",
        "The area of the house can be caculated from the frontage and the depth.\n",
        "\n",
        "$$\n",
        "area = frontage \\times depth\n",
        "$$\n",
        "\n",
        "You may have an intution that the area of the land is more predictive of the price than the frontage and depth as separate features, so you define a new feature:\n",
        "\n",
        "$$\n",
        "x_3 = x_1 \\times x_2\n",
        "$$\n",
        "\n",
        "This gives us a new possible model:\n",
        "\n",
        "$$\n",
        "f_{\\vec{w},b} = w_1x_1 + w_2x_2 + w_3x_3 + b\n",
        "$$"
      ],
      "metadata": {
        "id": "EoiSG99obJMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition: Feature engineering\n",
        "\n",
        "Using **intuition** to design **new features**, by transforming or combining original features."
      ],
      "metadata": {
        "id": "y2Ao2tM1gCkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.6 Polynomial regression\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1lt8-gP5BkAgBhAy-qfHVrGHxHRnqCKPo'>\n",
        "\n",
        "- This might work, but eventually a quadratic model will come down, which doesn't make sense for housing prices as the size increases.\n",
        " - A cubic function might work better:\n",
        "\n",
        " $$\n",
        " f_{\\vec{w},b} = w_1x + w_2x^2 + w_3x^3 + b\n",
        " $$\n",
        "\n",
        "- NOTE: as you create new features by squaring or cubing existing features, **feature scaling becomes increasingly important**\n",
        "- Here is another reasonable alternative:\n",
        "\n",
        "$$\n",
        "f_{\\vec{w},b} = w_1x + w_2\\sqrt{x} + b\n",
        "$$\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "pWkicFQThHdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.7 Lab - Feature engineering and Polynomial regression\n",
        "\n",
        "https://colab.research.google.com/drive/1lnKD6XZNOn1LOii-Iv8xYZcQ1F2oKTYe\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HyvJ02ixlyQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.8 Lab - Linear regression with scikit-learn\n",
        "\n",
        "https://colab.research.google.com/drive/1a2thuEs25-bG9bD-bc_o-KPoyDm5r8gx"
      ],
      "metadata": {
        "id": "PrG8-2Qslyh4"
      }
    }
  ]
}