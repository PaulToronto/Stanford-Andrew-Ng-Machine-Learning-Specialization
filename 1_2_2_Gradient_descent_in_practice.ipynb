{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsBYX/3OIHWydObrVmErqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/Stanford-Andrew-Ng-Machine-Learning-Specialization/blob/main/1_2_2_Gradient_descent_in_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2.2 Gradient descent in practice"
      ],
      "metadata": {
        "id": "mWM5zG2TYgrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ZD9KsPKEYlY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYTl9EyrYWKa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.1 Feature scaling"
      ],
      "metadata": {
        "id": "8T7DV4YiYnBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature and parameter values\n",
        "\n",
        "$$\n",
        "\\widehat{price} = w_1x_1 + w_2x_2 + b\n",
        "$$\n",
        "\n",
        "- price is in \\$1000s\n",
        "- $x_1$ represents the size of the house in $\\text{feet}^{2}$\n",
        " - range: 300 to 2,000 square feet\n",
        " - relatively **large range** of values\n",
        "- $x_2$ represents the nunber of bedrooms\n",
        " - range: 0 to 5 bedrooms\n",
        " - relatively **small range** of values\n",
        "\n",
        "#### Example House\n",
        "\n",
        "- House: $x_1 = 2000$, $x_2 = 5$, $price = 500$\n",
        "- $b = 50$\n",
        "- Possible parameters: $w_1 = 50, w_2 = 0.1$\n",
        " - $\\widehat{price} = 100,050,500$\n",
        " - clearly, not a good choice for $w_1$ and $w_2$\n",
        "- Possibe parameters: $w_1 = 0.1, w_2 = 50$\n",
        " - $\\widehat{price} = 500,000$\n",
        " - a perfect choice for $w_1$ and $w_2$\n",
        "\n",
        "#### What does this show?\n",
        "\n",
        "- When a feature has a relatively large range, there is a good chance that the optimal weight for that feature will be a small value.\n",
        "- Likewise, when the possible values of feature are small, there is a good change that the optimal weight for that feature will be a large value.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmrJ7do_-d4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does this relate to gradient descent?\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=198UySZdh8xm9dyfEWXQOzogdz1Gxdbh7'>"
      ],
      "metadata": {
        "id": "mCBd2HkxIVwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Notice that the contours form ellipses, not circles\n",
        "- Because the contours are so tall and skinny, gradient descent may end up bouncing back and forth for a long time before it can finally find its way to the global minimum\n",
        "- **Feature scaling** enables gradient descent to find a much more direct path to the global minimum."
      ],
      "metadata": {
        "id": "656iSjm0LiWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1zKhYXV1GtOJxBxjUrRgmp1nHyv1kkWn0'>"
      ],
      "metadata": {
        "id": "0Olgvd3iMpWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to implement scaling\n",
        "\n",
        "#### Method 1: Divide each value by the maximum\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "300 \\le x_1 \\le 2000 &\\rightarrow 0.15 \\le x_1 \\le 1 \\\\\n",
        "0 \\le x_2 \\le 5 &\\rightarrow 0 \\le x_2 \\le 1\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "#### Method 2: Mean normalization (centres it at 0)\n",
        "\n",
        "$$\n",
        "x_j = \\frac{x_j - \\mu_j}{max(x_j) - min(x_j)}\n",
        "$$\n",
        "\n",
        "#### Mathod 3: Z-score normalization\n",
        "\n",
        "$$\n",
        "x_j = \\frac{x_j - \\mu_j}{\\sigma_1}\n",
        "$$"
      ],
      "metadata": {
        "id": "opLMVZ5GOTc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling Examples\n",
        "\n",
        "- aim for about $-1 \\le x_j \\le 1$ for each feature $j$\n",
        "- the following are all ok:\n",
        " - $-3 \\le x_j \\le 3$\n",
        " - $-0.3 \\le x_j \\le 0.3$\n",
        " - $0 \\le x_j \\le 3$\n",
        " - $-2 \\le x_j \\le 0.5$\n",
        "- not ok, should rescale\n",
        " - $-100 \\le x_3 \\le 100$, too large\n",
        " - $-0.0001 \\le x_4 \\le 0.0001$, too small\n",
        " - $98.6 \\le x_5 \\le 105$, too large"
      ],
      "metadata": {
        "id": "A0f2YyuiSafN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.2.2 Checking gradient descent for convergence"
      ],
      "metadata": {
        "id": "LMVMlnk9UyYm"
      }
    }
  ]
}