{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOnj10YQLJajWclOyt3xmK7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/Stanford-Andrew-Ng-Machine-Learning-Specialization/blob/main/1_3_3_Gradient_Descent_for_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.3 Gradient Descent for Logistic Regresssion"
      ],
      "metadata": {
        "id": "z8Jczh5Nw3rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Nt7U8JgWw8_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.3.1 Gradient Descent Implementation"
      ],
      "metadata": {
        "id": "K-_NVFoZw-nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction\n",
        "\n",
        "$$\n",
        "f_{\\vec{w},b}\\left(\\vec{x}\\right) = \\frac{1}{1 + e^{-(\\vec{w}\\cdot\\vec{x} + b)}}\n",
        "$$"
      ],
      "metadata": {
        "id": "rKylgkl275i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "P(y = 1|\\vec{x};\\vec{w},b)\n",
        "$$"
      ],
      "metadata": {
        "id": "k4hhbDCB8eOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function\n",
        "\n",
        "$$\n",
        "J\\left(\\vec{w},b\\right) = -\\frac{1}{m}\\sum_{i=1}^{m}\\left(y^{(i)}\\log{\\left(f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right)\\right)} + \\left(1 - y^{(i)}\\right)\\log{\\left(1 - f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right)\\right)}\\right)\n",
        "$$"
      ],
      "metadata": {
        "id": "kebf-jzY825V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "Don't forget to do the updates simultaneously.\n",
        "\n",
        "repeat {\n",
        "$$\n",
        "\\begin{align}\n",
        "w_j &= w_j - \\alpha \\frac{\\partial}{\\partial w_j}J\\left(\\vec{w}, b\\right) \\\\\n",
        "b &= b - \\alpha \\frac{\\partial}{\\partial b}J\\left(\\vec{w}, b\\right)\n",
        "\\end{align}\n",
        "$$\n",
        "}"
      ],
      "metadata": {
        "id": "j82VvBxL9i9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Gradient\n",
        "\n",
        "The equations for the logistic regression gradients look exactly the same as the equations used for the linear regression gradients, but the definition of $f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right)$ has changed."
      ],
      "metadata": {
        "id": "De2ao9bIPb1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^{m}\\left(f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right) - y^{(i)}\\right)x_j^{(i)} \\\\\n",
        "\\frac{\\partial}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^{m}\\left(f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right) - y^{(i)}\\right)\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "vRavfrQ4PisX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concepts that apply to gradient descent for linear regression also apply here:\n",
        "\n",
        "- Monitor the gradient descent learning curve to make sure it is converging\n",
        "- There is a vectorized implementation\n",
        "- Feature scaling can help gradient descent coverge faster"
      ],
      "metadata": {
        "id": "FZpJw_cHTZxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.3.2 Lab - Gradient descent for logistic regression\n",
        "\n",
        "https://colab.research.google.com/drive/1WfW2UehNdCORcvo9r2OYPGcOlDOQ38MQ"
      ],
      "metadata": {
        "id": "1xW8vmp3VU9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.3.3 Lab - Logistic Regression with scikit-learn\n",
        "\n",
        "https://colab.research.google.com/drive/1jcabFCA5XZZXDIQE4q8J0iiMovAju9Of"
      ],
      "metadata": {
        "id": "Qp7TnRj6VauM"
      }
    }
  ]
}