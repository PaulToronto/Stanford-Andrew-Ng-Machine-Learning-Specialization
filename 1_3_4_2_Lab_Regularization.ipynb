{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPsC52zuIUbIr82PLuBI3ig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/Stanford-Andrew-Ng-Machine-Learning-Specialization/blob/main/1_3_4_2_Lab_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab - Regularization"
      ],
      "metadata": {
        "id": "1cy7k-zgtV1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "godHKi2ltZR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6WF9pAQgtQ_L"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "dn3LnPqCq1R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z : array_like\n",
        "        A scalar or numpy array of any size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "     g : array_like\n",
        "         sigmoid(z)\n",
        "    \"\"\"\n",
        "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
        "    g = 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "amhMRh1iq2mB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Functions with Regularization"
      ],
      "metadata": {
        "id": "F90nMToXh55h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function for Regularized Linear Regression"
      ],
      "metadata": {
        "id": "ao4B_5gaiJIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2\n",
        "$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$"
      ],
      "metadata": {
        "id": "OagfZ1rqibiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b\n",
        "        cost += (f_wb_i - y[i])**2\n",
        "    cost /= (2 * m)\n",
        "\n",
        "    reg_cost = 0.0\n",
        "    for j in range(n):\n",
        "        reg_cost += w[j]**2\n",
        "    reg_cost = (lambda_/(2 * m)) * reg_cost\n",
        "\n",
        "    total_cost = cost + reg_cost\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "3GAC_JMgjKcj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5, 6)\n",
        "y_tmp = np.array([0, 1, 0, 1, 0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1, ) - 0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "\n",
        "compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCL417qrjcfE",
        "outputId": "fccb02aa-5529-49d1-9b47-33345cf1b82d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07917239320214277"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function for Regularized Logistic Regression"
      ],
      "metadata": {
        "id": "nOS0MxM2pzD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2\n",
        "$$\n",
        "where:\n",
        "$$\n",
        "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)\n",
        "$$"
      ],
      "metadata": {
        "id": "ZaxNS_LBp3E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i], w) + b\n",
        "        f_wb_i = sigmoid(z_i)\n",
        "        cost += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)\n",
        "    cost /= m\n",
        "\n",
        "    reg_cost = 0.0\n",
        "    for j in range(n):\n",
        "        reg_cost += w[j]**2\n",
        "    reg_cost = (lambda_/(2 * m)) * reg_cost\n",
        "\n",
        "    total_cost = cost + reg_cost\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "6FvsbtABqHvA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5, 6)\n",
        "y_tmp = np.array([0, 1, 0, 1, 0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1, ) - 0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "\n",
        "compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0r6EOPgr2Mi",
        "outputId": "6e665026-87c8-420b-bcaf-80e17522d156"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6850849138741673"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent with Regularization\n",
        "\n",
        "The basic algorithm for running gradient descent does not change with regularization, it is:\n",
        "$$\\begin{align*}\n",
        "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
        "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\\n",
        "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
        "&\\rbrace\n",
        "\\end{align*}$$\n",
        "Where each iteration performs simultaneous updates on $w_j$ for all $j$.\n",
        "\n",
        "What changes with regularization is computing the gradients."
      ],
      "metadata": {
        "id": "SMum8QMws2P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Gradient with Regularization\n",
        "\n",
        "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$.\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "For linear regression:\n",
        "$$\n",
        "f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
        "$$  \n",
        "\n",
        "For logistic regression:\n",
        "$$\n",
        "z = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
        "$$  \n",
        "    \n",
        "$$\n",
        "f_{\\mathbf{w},b}(x) = g(z)\n",
        "$$  \n",
        "\n",
        "where $g(z)$ is the sigmoid function:  \n",
        "$$\n",
        "g(z) = \\frac{1}{1+e^{-z}}\n",
        "$$   "
      ],
      "metadata": {
        "id": "Y7ZZXTJTsYdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient for Linear Regression"
      ],
      "metadata": {
        "id": "elk1y33mxMeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_linear_reg(X, y, w, b, lambda_):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.0\n",
        "\n",
        "    for i in range(m):\n",
        "        err = np.dot(X[i], w) + b - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] += err * X[i, j]\n",
        "        dj_db += err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] += (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "f87dERAxtzFu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "dj_db_tmp, dj_dw_tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSlqAeH8v_Ig",
        "outputId": "35b58eaa-a0fa-4537-b49c-2d0eb1785f2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6648774569425726, array([0.29653215, 0.49116796, 0.21645878]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient for Logistic Regression"
      ],
      "metadata": {
        "id": "AgzuhWhrxVGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic_reg(X, y, w, b, lambda_):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns\n",
        "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.0\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i], w) + b)\n",
        "        err = f_wb_i - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] += err * X[i, j]\n",
        "        dj_db += err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] += (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "IlCkmFIyxkFT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "dj_db_tmp, dj_dw_tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVEIvL2dyCOU",
        "outputId": "5f432923-1dc4-4e51-c98d-4659033e4a65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.341798994972791, array([0.17380013, 0.32007508, 0.10776313]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}