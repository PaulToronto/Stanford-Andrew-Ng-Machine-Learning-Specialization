{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO4yC5CIrSpc4mEW38N5Pfu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/Stanford-Andrew-Ng-Machine-Learning-Specialization/blob/main/1_3_4_The_problem_of_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.4 The problem of overfitting"
      ],
      "metadata": {
        "id": "Bmizq1aHsb61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.4.1 The problem of overfitting"
      ],
      "metadata": {
        "id": "daEPY8tvsl-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Example\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1g73vbNNJVusL9Rht4lG91wKFPVOcXS1a'>\n",
        "\n",
        "- **high bias** refers to a strong preconception or bias that the housing prices are going to be a completely linear function\n",
        "- **high variance** refers to the fact that a slightly different training set would likely give rise to a very different model"
      ],
      "metadata": {
        "id": "6XHrl8kemsMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Example\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1pptyAySCxsikh1qlU6ZZ_n8mW1NmZjtj'>\n",
        "\n"
      ],
      "metadata": {
        "id": "JYiBjHa6r20u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.4.2 Addressing overfitting"
      ],
      "metadata": {
        "id": "gYGheVSWt3iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ways to address overfitting\n",
        "\n",
        "1. Collect more training examples\n",
        "    - this should be tried first, when possible\n",
        "2. Select features\n",
        "    - remove polynomial features\n",
        "    - just use a subset of the available features\n",
        "        - keeping the most relevant features\n",
        "        - this is called feature selection\n",
        "        - use your intuition\n",
        "    - **disadvantage** to this is that you are losing information\n",
        "    - in the next course we learn more about how to choose the best features\n",
        "3. Regularization\n",
        "    - reduce size of parameters\n",
        "    -  setting a parameter to 0 is equivalent to eliminating a feature\n",
        "    - regularization, which encourages the algorithm to shrink the values of parameters, is gentler alternative to eliminating a feature\n",
        "    - by convention, regularization works on $w_j$ regularizing $b$ doesn't usually make a big difference\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1aSoXIGqF6rGwwrTxYjZpD80vh_r_NxIY'>"
      ],
      "metadata": {
        "id": "MzXEYu6WMdC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.4.3 Lab - Overfitting\n",
        "\n",
        "https://colab.research.google.com/drive/1WuDJ8wVA3InRJf76sQQmnOqjT0MCsPY4"
      ],
      "metadata": {
        "id": "6hCaBo5tSSVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.4.4 Cost function with regularization\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1_jCe8DF0G749UyuuZyQacSEn0NtBl2RO'>\n",
        "\n",
        "- 1000 is chosen here just because it is a big number\n",
        "- with this modified cost function, the model is penalized if $w_3$ and $w_4$ are large\n",
        "    - because if you want to minimize this function, the only way to do is is if $w_3$ and $w_4$ are small\n",
        "    - when this function is minimized you will end up with $w_3$ and $w_4$ close to 0\n",
        "    - the result is a fit to the data that is much closer to the quadratic function, including a small contribution from features $x^3$ and $x^4$"
      ],
      "metadata": {
        "id": "RUm_9vvoSjnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?export=view&id=1mk7E3PWazGDlGnhbIirPFXAVPDkwtAe9'>\n",
        "\n",
        "- having small values for the parameters: $w_1, w_2, \\cdots, w_n, b$ is a bit like having a simpler model\n",
        "- in the last slide we only regularized (penalized) $w_3$ and $w_4$, but in practice you may not know which are the most important features, so you won't know which features to regularize, so typically, you'd regularize all the $w_j$ parameters\n",
        "- it is possible to show that this results in fitting a smoother, simpler, less wiggly function that is less prone to overfitting\n",
        "- $\\lambda$ is called the **regularization parameter**\n",
        " - $\\lambda \\gt 0$\n",
        "- like the first part of the cost function, the regularization part is scalled by $\\frac{1}{2m}$\n",
        " - by matching the scaling it becomes easier to choose a good value for $\\lambda$\n",
        " - makes it more likely that the chosen value for $\\lambda$ will continue to work even after adding additional training examples\n",
        "- by convention, $b$ is not regularized\n"
      ],
      "metadata": {
        "id": "WV7228Av0NnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?export=view&id=1df43iYp_6NvatUqfe-IR0CeUNQrxiBzp'>\n",
        "\n",
        "- the choice of $\\lambda$ is how you balance between the two goals\n",
        " 1. fitting the data\n",
        " 2. keeping $w_j$ small\n",
        "- when $\\lambda = 0$ you are not using the regularization term at all\n",
        "- when $\\lambda = 10^{10}$ you are placing a very heavy weight on the regularization term and in that case, the only way to minimize the cost function is to have $w_j$ values very clost to 0\n",
        " - This results a horizontal line: $f_{\\vec{w},b}(\\vec{x}) = b$"
      ],
      "metadata": {
        "id": "XfBPVvuI-Teg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.4.5 Regularized linear regression"
      ],
      "metadata": {
        "id": "wsUrUam5BveY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function for Regularized Linear Regression\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=1}^{n} w_j^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b\n",
        "$$"
      ],
      "metadata": {
        "id": "rrUolYP-CDNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "The goal is to find parameters $w$ and $b$ to minimize the cost function.\n",
        "\n",
        "$$\n",
        "\\text{minimize}_{w,b}\\;J(w,b)\n",
        "$$\n",
        "\n",
        "\n",
        "repeat {\n",
        "$$\n",
        "\\begin{align}\n",
        "w_j &= w_j - \\alpha \\cdot \\frac{\\partial}{\\partial w_j} J(\\vec{w},b) \\\\\n",
        "b &= b - \\alpha \\cdot \\frac{\\partial}{\\partial b} J(\\vec{w},b)\n",
        "\\end{align}\n",
        "$$\n",
        "}\n",
        "\n",
        "### The Gradients\n",
        "\n",
        "It is not necessary to regularize $b$.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 1}^{} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 1}^{m} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "HzAB6nZRCjzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Another way to consider the simultaneous updates of $w_j$ in gradient descent\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "w_j &= w_j - \\alpha\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\left(f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right) - y^{(i)}\\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j\\right) \\\\\n",
        "&= 1w_j - \\alpha\\frac{\\lambda}{m}w_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}\\left(f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right) - y^{(i)}\\right)x_j^{(i)} \\\\\n",
        "&= w_j\\left(1 - \\alpha\\frac{\\lambda}{m}\\right) - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}\\left(f_{\\vec{w},b}\\left(\\vec{x}^{(i)}\\right) - y^{(i)}\\right)x_j^{(i)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "- the second term is the term we've seen before\n",
        "- the first term used to be $w_j$ but is now the following:\n",
        "\n",
        "$$\n",
        "w_j\\left(1 - \\alpha\\frac{\\lambda}{m}\\right)\n",
        "$$\n",
        "\n",
        "- $\\alpha$ is a very small positive number, say 0.01\n",
        "- $\\lambda$ is usually a small positive number, say 1 or 10\n",
        "- $m$ is a positive number, say 50"
      ],
      "metadata": {
        "id": "rp7gRG-USuCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.01\n",
        "lmbda = 1\n",
        "m = 50\n",
        "\n",
        "1 - alpha * (lmbda/m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3xQJ6LTZqGw",
        "outputId": "1aafd0cd-2157-4f31-9358-0470ca2d8503"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9998"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- essentially, we are multiply $w_j$ by 0.9998, a number slightly less than 1\n",
        "- this makes $w_j$ smaller"
      ],
      "metadata": {
        "id": "QrWpb8Q2aF4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Derivative Term"
      ],
      "metadata": {
        "id": "C22xy3xoa64Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j}J\\left(\\vec{w},b\\right) &= \\frac{\\partial}{\\partial w_j}\\left[\\frac{1}{2m} \\sum_{i = 1}^{m} \\left(f_{\\vec{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2  + \\frac{\\lambda}{2m}  \\sum_{j=1}^{n} w_j^2\\right] \\\\\n",
        "&= \\frac{\\partial}{\\partial w_j}\\left[\\frac{1}{2m} \\sum_{i = 1}^{m} \\left(\\vec{w} \\cdot \\vec{x} + b - y^{(i)}\\right)^2  + \\frac{\\lambda}{2m}  \\sum_{j=1}^{n} w_j^2\\right] \\\\\n",
        "&= \\frac{1}{2m} \\sum_{i-1}^{m}\\left[\\left(\\vec{w}\\cdot\\vec{x} + b - y^{(i)}\\right)2x_j^{(i)}\\right] + \\frac{\\lambda}{2m}2 w_j \\\\\n",
        "&= \\frac{1}{m}\\sum_{i=1}^{m}\\left[\\left(\\vec{w}\\cdot\\vec{x} + b - y^{(i)}\\right)x_j^{(i)}\\right] + \\frac{\\lambda}{m}w_j \\\\\n",
        "&= \\frac{1}{m}\\sum_{i=1}^{m}\\left[\\left(f\\left(\\vec{x}^{(i)}\\right) - y^{(i)}\\right)x_j^{(i)}\\right] + \\frac{\\lambda}{m}w_j\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "I8LPyGKhlUv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.4.6 Regularized logistic regression"
      ],
      "metadata": {
        "id": "eotKIbd2mx36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1J2fFjqSF79ZIagEWRTHHUxqRxeh0qxTF'>\n",
        "\n",
        "- In general, when training a logistic regression model with a lot of features (polynomial or not), there is a high risk of overfitting"
      ],
      "metadata": {
        "id": "pmv7NT44wefO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1sCTDSGzSuR6aAZVwt3Q7rC8x6Kwek2hF'>\n",
        "\n",
        "- the gradient for regularized logistic regression looks exactly the same as that for regularized regression\n",
        "    - the only difference is that $f$ is no longer a linear function\n",
        "    - it is the logistic functinm applied to $z$\n",
        "- as with regularized linear regression, the convention is to only regularize the $w_j$ parameters, not $b$."
      ],
      "metadata": {
        "id": "RY2seuglzLDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.4.7 Lab - Regularization\n",
        "\n",
        "https://colab.research.google.com/drive/1NGGCi42u6B0uyIj7mrcSg8ygIWpuD2rB"
      ],
      "metadata": {
        "id": "ZRFUiJdf3S98"
      }
    }
  ]
}