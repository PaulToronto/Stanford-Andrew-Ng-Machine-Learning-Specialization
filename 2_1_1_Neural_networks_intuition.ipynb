{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMJsC8biBW47eXZsCmuDNz5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/Stanford-Andrew-Ng-Machine-Learning-Specialization/blob/main/2_1_1_Neural_networks_intuition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.1 Neural networks intuition"
      ],
      "metadata": {
        "id": "TgTnYEdoONAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1.1 Welcome\n",
        "\n",
        "1. Week 1: Neural Networks, inference (prediction)\n",
        "2. Week 2: Neural Networks, training\n",
        "3. Week 3: Practical advice for building machine learning systems\n",
        "4. Week 5: Decision Trees\n"
      ],
      "metadata": {
        "id": "vdfd74lUQ2q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1.2 Neurons and the brain\n",
        "\n",
        "- **Neural networks** are sometimes called **artificial neural networks**\n",
        "- Origins: Algorithms that try to mimic the brain\n",
        "- Started in the 1950s and then fell out of favour\n",
        "- Used in the 1980s and early 1990s\n",
        "    - mostly for handwritten digit recognition\n",
        "        - mail postal codes and checks\n",
        "- Resurgence from around 2005\n",
        "    - rebranded as **deep learning**\n",
        "    - speech recognition\n",
        "    - computer vision\n",
        "    - ImageNet (2012): https://image-net.org/\n",
        "    - text, natural language processing\n",
        "    - now they are used in a wide variety of application areas\n",
        "- Even though today's neural networks have almost nothing to do with how the brain learns, there was the early motivation of trying to build software that mimics the brain"
      ],
      "metadata": {
        "id": "zXJPTgtsRryp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does the brain work?\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=10jGJcVuQOS7r-j0Jc1N2n_Rq4MFcqvLd'>\n",
        "\n",
        "- All of human thought is from neurons sending electrical impulses and sometimes forming new connections with other neurons\n",
        "- Neurons receive electrical impulses from other neurons, then the neuron carries out some computations and send the output to other neurons\n",
        "- The output of one neuron beccomes the input to other neurons\n",
        "- Neurons aggrgegate imputs from multiple other neurons and then maybe sends that output to yet other neurons\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1CdedwBQZBQIFrCZxAN5B7c9KYKO9zA3Q'>\n",
        "\n",
        "- The input \"wires\" are called **dendrites**\n",
        "- Neurons occassionally send electrical impulses to other neurons via the output wire which is called the **axon**\n",
        "<hr/>\n",
        "- In an artificial neural network, a neuron takes one or more inputs which are just numbers\n",
        "- It does some computation, outputs a number which can become input to another neuron\n",
        "- Often many neurons are simulated at the same time\n",
        "- **BIG CAVEAT**: the analogy between artificial neural networks and biological neurons is a loose one. We have almost no idea how the human brain works\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PeZ1F3LMR0Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Now?\n",
        "\n",
        "- The ideas of neural networks have been around for decades\n",
        "- The answer to this question is best answered with a diagram\n",
        "- The amount of data we have is increasing\n",
        "- We have faster computer processors\n",
        "- We have **GPU**s\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1FNRmsRKkVFbI9SK3z-cW8fvtKmvyqfn8'>\n",
        "\n"
      ],
      "metadata": {
        "id": "rGH4ij8ra3WS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1.3 Demand Prediction"
      ],
      "metadata": {
        "id": "0QvS_m06clgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Will this product be a top seller or not?\n",
        "\n",
        "- selling t-shirts\n",
        "- you have collected data of different t-shirts that were sold at different prices, as well as which ones became a top seller\n",
        "- this type of application is used by retailers today for inventory planning and marketing campaigns\n",
        "- previously, we tackled problems like this with logistic regression\n",
        "- to set this up to **build a neural network** we switch the terminolgy a bit and use $a$ to denote the output rether than $f(x)$\n",
        "    - $a$ stands for **activation**\n",
        "    - activation is a term from neuroscience and it refers to how much a neuron is sending a high output to other neurons downstream from it\n",
        "- it turns out that this little logistic regression algorithm canm be though of as a very simplified model of a single neuron in the brain\n",
        "    1. the neuron takes as input the price, $x$\n",
        "    2. it computes the forumula\n",
        "    3. it's output is $a$, which is the probability of the t-shirt being a top seller\n",
        "- can think of a neuron as being a tiny computer whose only job is to take a number (or a few numbers), such as price, then to output one number, such as the probability of being a top seller\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1mYnBF_ptr5kSBYtlyLtK4dWz2FbQO64O'>\n",
        "\n",
        "- Building a neural network just requires taking a bunch of these neurons and \"wiring\" them together\n",
        "\n"
      ],
      "metadata": {
        "id": "Vg2l2lvQc9Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A more complex example of demand prediction\n",
        "\n",
        "Now we have four input features:\n",
        "\n",
        "- price of the t-shirt\n",
        "- shipping cost\n",
        "- marketing amount\n",
        "- material quality\n",
        "\n",
        "You might suspect that whether or not a t-shirt becomes a best seller depends on a few factors:\n",
        "\n",
        "- affordability\n",
        "- awareness\n",
        "- perceived quality (bias or potential bias that this is a high quality t-shirt)\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-j2ke6ibyXIncdwDGr1ToMwh9e-p6VMO'>\n",
        "\n",
        "- here, we manually decided which neurons should take which inputs as features (i.e. affordability is a function of price and shipping costs)\n",
        "- in practice, each neuron in a certain layer, say the layer in the middle, will have access to every feature, to every value in the previous layer\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1oasWyw01avy6Bm_JvNnxl9gwhD7BAoSE'>\n",
        "\n",
        "- the layer for \"affordability\" may learn to ignore marketing and material to only focus on the features that are most relevant to affordablity\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1VH1Vk9hd_WXUH_ZsE_xVLsYmiMx5w27Z'>\n",
        "\n",
        "- further simplifying the notation, the four input features are are written as a vector, $\\vec{x}$, which is the **input layer**\n",
        "- this feature vector is fed to the layer in the middle, called the **hidden** layer, which then computes three **activation values**\n",
        "- these 3 numbers become another vector, $\\vec{a}$\n",
        "- the vector, $\\vec{a}$ is fed to the **output layer**, which finally outputs the probability of this t-shirt becoming a top seller, $a$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qndYUSTCg6qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Another of thinking about neural networks\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1hfL7alVXwIJrW1V_Fs-aj99XeXN6hG0D'>\n",
        "\n",
        "- covering up the left half of the diagram\n",
        "- what we are left with is a **logistic regression algorithm** or **logistic regression unit** that is taking as input, afforability, awareness and perceived quality and using these 3 features to estimate the probability of the t-shirt being a top seller\n",
        "- this is just logistic regression, but the cool thing about it is that rather than using the original features, price, shipping cost, marketing and material we are using what is maybe a better set of features which are hopefully more predictive of whether or not the t-shirt will be a top seller\n",
        "    - a version of logistic regression that can learn its own feature that make it easier to make accurate predictions\n",
        "    - in the previous course we multiplied the frontage by the depth to engineer a new feature, that was manual feature engineering"
      ],
      "metadata": {
        "id": "xrI53zALs6kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of what a neural network does\n",
        "\n",
        "- the input layer has a vector of features, four numbers in our example, $\\vec{x}$\n",
        "- this vector of length 4 is input to the hidden layer, which outputs a vector of length 3\n",
        "    - this is a vector of activations, $\\vec{a}$\n",
        "- this vector of length 3 outputs one number, $a$\n",
        "\n",
        "### Important\n",
        "\n",
        "- In our example we explicitly decided that neural network should compute affordability, awareness and perceived quality\n",
        "- In practice, the neural network decides what should be computed for its hidden layer(s)"
      ],
      "metadata": {
        "id": "CA2jIGARxPoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A neural network can have multiple hidden layers\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=10yryONtvApbaU9ibDXr_mt37tScqSa2J'>\n",
        "\n",
        "- When building a neural network, one of the decisions you need to make is how many hidden layers you want and how many neurons in each layer\n",
        "- This is a question of the architecture of the neural network\n",
        "- Some tips for choosing an appropriate architecture of the neural network are covered later in this course\n",
        "    - the choice of architecture has an impact on the performance of the neural network\n",
        "- A neural network with mutiple layers is called a **multilayer perceptron**"
      ],
      "metadata": {
        "id": "UDisG62Mzmfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1.4 Example: Recognizing Images"
      ],
      "metadata": {
        "id": "d3AVz7Zm16gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1aiJWM9A0lXWJb74zrpNZPowdAl4BiGwd'>\n",
        "\n",
        "- the values in the matrix are pixel brightness values\n",
        "\n"
      ],
      "metadata": {
        "id": "TTUhQK5a237o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=18oTKdunNp9nI53oKHaNZLE-g2rfb5GU1'>\n",
        "\n",
        "- Suppose a neural network has been trained on a lot of images of faces\n",
        "- If you pieek at the different neurons in the hidden layers to figure out what they may be computing, this is what you might find\n",
        "    - the first layer could be finding very short lines or edges in the image\n",
        "    - in the second layer, the neurons might learn to group together lots of little short edge segments in order to look for parts of faces\n",
        "        - the first neuron appears to be detecting the presence or absence of an eye in a certain position of the image\n",
        "        - the second neuron, the corner of a nose\n",
        "        - the bottom of an ear\n",
        "        - ...\n",
        "    - in the next layer the neural network is aggregating different parts of faces to then try to  detect the presence or absence of larger, coarser face shapes\n",
        "    - this helps the output layer detect the identity of the person\n",
        "- Each successive layer loks at large and larger windows\n",
        "    - this little neuron visualizations actually correspond to differently sized regions in the image"
      ],
      "metadata": {
        "id": "eXsz0GWH5C6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Car Classification\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1YSA66tkk-0Pg8TMsdDMS2sNgDUrNpjt2'>\n",
        "\n",
        "- Just by feeing it different data, the neural network learns to detect very different features"
      ],
      "metadata": {
        "id": "82ZTWKSc8uKg"
      }
    }
  ]
}