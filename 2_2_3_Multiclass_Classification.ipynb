{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOG/oAGB0yA+W040m6lS9c7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/Stanford-Andrew-Ng-Machine-Learning-Specialization/blob/main/2_2_3_Multiclass_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2.3 Multiclass Classification"
      ],
      "metadata": {
        "id": "AVlHJXsdqPox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3.1 Multiclass\n",
        "\n",
        "Target, $y$, can take on **more than two** possible values\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1akw0YgvtT5dRBMH1kX1-n4FEyXuhnLTL'>\n",
        "\n",
        "### Examples\n",
        "\n",
        "- MNIST: could be any one of 10 digits\n",
        "- Trying to classify whether patients have any of five different possible diseases\n",
        "- Visual inspection of part defects in a factory\n",
        " - scratch, discoloration, chip defect"
      ],
      "metadata": {
        "id": "rXLmAXBiqVwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3.2 Softmax"
      ],
      "metadata": {
        "id": "nU9rLXDPube4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Softmax regression algorithm** is a generalization of **logistic regression**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=10lKA9fxtJWQIENs-LXKkIxajuut25eec'>\n",
        "\n",
        "- If you apply softmax regression with $n = 2$ it computes the same thing as logistic regression.\n",
        " - The parameters end up being a little bit different, but it reduces to a logistic regression model"
      ],
      "metadata": {
        "id": "benuh9wMviwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Cost Function\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1itSB-E4kiInMQkOClEznl0uuGunvZpTW'>"
      ],
      "metadata": {
        "id": "Cpqcjy0-0Oe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3.3 Neural Network with Softmax output\n",
        "\n",
        "Essentially, we take the Softmax regression model and put it into the output layer of a neural network."
      ],
      "metadata": {
        "id": "-6d93HYw0wKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we did handwritten digit recognition with just two classes.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1JDkX0dONTpAPLr6SaqEmvSaN1C-l3UzJ'>\n",
        "\n",
        "We can modify this to accomodate 10 digits.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-OjamBEZL1J7vkCIl-CgM3-MWoO1Dig0'>\n",
        "\n",
        "\n",
        "- The softmax layer or activation function is different from the other activation functions\n",
        " - With the other activation functions, $a_n$ is a function of $z_n$ and only $z_n$\n",
        "   - $a_n = g\\left(z_n\\right)$\n",
        "   - that means we can apply the activation function element-wise\n",
        " - With softmax $a_n$ is a function of all of $z_1, z_2, \\cdots, z_n$ **simultaneously**\n",
        "   - each of the activation values depend on all of the values of $z$\n",
        "   - we can no longer apply the activation function element-wise"
      ],
      "metadata": {
        "id": "9LX_0jX94fm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to implement Softmax in TensorFlow\n",
        "\n",
        "- **NOTE: don't actually use this code, there is a better version, covered in the next section**\n",
        "- As before, there are 3 steps\n",
        "- The loss function is `SparseCategoricalCrossentropy()`\n",
        " - `Sparse` means that the categories are mutually exclusive. A digit can't be a 4 and a 5 simultaneously\n",
        "\n",
        " <img src='https://drive.google.com/uc?export=view&id=1bzIY-6_3DMFDd-W7xMvBMTVetXlvkyFn'>"
      ],
      "metadata": {
        "id": "hqwetJws_HY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3.4 Improved implementation of Softmax"
      ],
      "metadata": {
        "id": "UUmhJHtoBlLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Roundoff Errors\n",
        "\n",
        "#### Option 1\n",
        "\n",
        "$$\n",
        "x = \\frac{2}{10000}\n",
        "$$\n",
        "\n",
        "#### Option 2\n",
        "\n",
        "$$\n",
        "x = \\left(1 + \\frac{1}{10000}\\right) - \\left(1 - \\frac{1}{10000}\\right)\n",
        "$$"
      ],
      "metadata": {
        "id": "-8_4VO3mWACR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "2 / 10_000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps63XyX4WYCN",
        "outputId": "366593f3-53aa-469a-bbd9-e4ccecae6e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(1 + 1/10_000) - (1 - 1/10_000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ74b9UeWc8n",
        "outputId": "01d1213e-29ee-492c-c920-f9f529937f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00019999999999997797"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Option 2 has a roundoff error"
      ],
      "metadata": {
        "id": "XV7755HFWoKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More numerically accurate implementation of logistic loss"
      ],
      "metadata": {
        "id": "Jh06PzgnXAud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=14SB2ci9facqQIs_gaUwG0B45glT4icA7'>\n",
        "\n",
        "- Original loss:\n",
        "    - for logistic regression this works ok, and usually, the roundoff error is not too great\n",
        "- More accurate loss:\n",
        "    - note in this version there is no insistance on computing an intermediate value explicitly\n",
        "    - this gives TensorFlow more flexibility in terms of how to do the computation\n",
        "- Note the changes to the TensorFlow code:\n",
        "    1. `loss=BinaryCrossentropy(from_logits=True)`\n",
        "    2. In the final layer, `activation='linear'`"
      ],
      "metadata": {
        "id": "9D-2eRd4X_Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Softmax, the roundoff errors become more serious\n",
        "\n",
        "- This is particularly true when $z$ is really small or really large\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1vUAskmkS8IosowqYT3XyG53-RiYKANUw'>\n",
        "\n",
        "- IMPORTANT: With the more accurate version, the model no longer outputs $a_1, \\cdots a_10$, instead it outputs $z_1, \\cdots, z_{10}$\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1vBYuQch3p2I0pvYHRmJeqJYkEnNYTtC_'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Rt2JVgOBb0uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3.5 Classification with multiple outputs\n",
        "\n",
        "### Multi-label Classification\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1W76-dglbRlhfgC8i3DD8Lpm3JFyKzvJK'>\n",
        "\n",
        "- One way to handle this problem is to just treat it as 3 separate machine learning problems and build 3 neural networks\n",
        "    - This is not an unreasonable approach, but there is another way\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=106xGoOhw4jW8UBSg5Jfj1pbfe8ofkJnu'>\n",
        "\n",
        "- Note that `sigmoid` can be used since this is a binary classification problem for all 3\n",
        "\n"
      ],
      "metadata": {
        "id": "aEVoPTs2fDTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3.6 Lab - Softmax\n",
        "\n",
        "https://colab.research.google.com/drive/1wkkYfpsImPmYdVB4h4uz7JsVZMH5QBl2"
      ],
      "metadata": {
        "id": "rlXX7Y2Pj9pE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3.7 Lab - Multiclass\n",
        "\n",
        "https://colab.research.google.com/drive/1zBhYkfsFZ-LW7foakf0oc0nvg3W1_ks7"
      ],
      "metadata": {
        "id": "qh7lhTnfj_P6"
      }
    }
  ]
}